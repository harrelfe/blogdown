<!-- BLOGDOWN-HEAD -->
<!-- /BLOGDOWN-HEAD -->


<p>Randomized clinical trials (RCT) have long been held as the gold standard for generating evidence about the effectiveness of medical and surgical treatments, and for good reason. But I commonly hear clinicians lament that the results of RCTs are not generalizable to medical practice, primarily for two reasons:</p>
<ol style="list-style-type: decimal">
<li>Patients in clinical practice are different from those enrolled in RCTs</li>
<li>Drug adherence in clinical practice is likely to be lower than that achieved in RCTs, resulting in lower efficacy.</li>
</ol>
<p>Point 2 is hard to debate because RCTs are run under protocol and research personnel are watching and asking about patients’ adherence (but more about this below). But point 1 is a misplaced worry in the majority of trials. The explanation requires getting to the heart of what RCTs are really intended to do: provide evidence for <strong>relative</strong> treatment effectiveness. There are some trials that provide evidence for both relative and absolute effectiveness. This is especially true when the efficacy measure employed is absolute as in measuring blood pressure reduction due to a new treatment. But many trials use binary or time-to-event endpoints and the resulting efficacy measure is on a relative scale such as the odds ratio or hazard ratio.</p>
<p>RCTs of even drastically different patients can provide estimates of relative treatment benefit on odds or hazard ratio scales that are highly transportable. This is most readily seen in subgroup analyses provided by the trials themselves - so called forest plots that demonstrate remarkable constancy of relative treatment benefit. When an effect ratio is applied to a population with a much different risk profile, that relative effect can still fully apply. It is only likely that the absolute treatment benefit will change, and it is easy to estimate the absolute benefit (e.g., risk difference) for a patient given the relative benefit and the absolute baseline risk for the subject. This is covered in detail in <a href="http://fharrell.com/links">Biostatistics for Biomedical Research</a>, Section 13.6. See also Stephen Senn’s excellent <a href="https://www.slideshare.net/StephenSenn1/real-world-modified">presentation</a>.</p>
<p>Clinical practice provides anecdotal evidence that biases clinicians. What a clinician sees in her practice is patient i on treatment A and patient j on treatment B. She may remember how patient i fared in comparison to patient j, not appreciate confounding by indication, and suppose this provides a valid estimate of the difference in effectiveness in treatment A vs. B. But the real therapeutic question is how does the outcome of a patient were she given treatment A compare to her outcome were she given treatment B. The gold standard design is thus the randomized crossover design, when the treatment is short acting. Stephen Senn eloquently <a href="http://onlinelibrary.wiley.com/doi/10.1002/sim.6739/abstract">writes</a> about how a 6-period 2-treatment crossover study can even do what proponents of personalized medicine mistakenly think they can do with a parallel-group randomized trial: estimate treatment effectiveness for individual patients.</p>
<p>For clinical practice to provide the evidence really needed, the clinician would have to see patients and assign treatments using one of the top four approaches listed in the hierarchy of evidence below. Entries are in the order of strongest evidence requiring the least assumptions to the weakest evidence. Note that crossover studies, when feasible, even surpass randomized studies of matched identical twins in the quality and relevance of information they provide.</p>
<p>Let <span class="math inline">\(P_{i}\)</span> denote patient <span class="math inline">\(i\)</span> and the treatments be denoted by <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. Thus <span class="math inline">\(P_{2}^{B}\)</span> represents patient 2 on treatment <span class="math inline">\(B\)</span>. <span class="math inline">\(\overline{P}_{1}\)</span> represents the average outcome over a sample of patients from which patient 1 was selected.</p>
<table style="width:38%;">
<colgroup>
<col width="12%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th>Design</th>
<th>Patients Compared</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>6-period crossover</td>
<td><span class="math inline">\(P_{1}^{A}\)</span> vs <span class="math inline">\(P_{1}^{B}\)</span> (directly measure HTE)</td>
</tr>
<tr class="even">
<td>2-period crossover</td>
<td><span class="math inline">\(P_{1}^{A}\)</span> vs <span class="math inline">\(P_{1}^{B}\)</span></td>
</tr>
<tr class="odd">
<td>RCT in idential twins</td>
<td><span class="math inline">\(P_{1}^{A}\)</span> vs <span class="math inline">\(P_{1}^{B}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\parallel\)</span> group RCT</td>
<td><span class="math inline">\(\overline{P}_{1}^{A}\)</span> vs <span class="math inline">\(\overline{P}_{2}^{B}\)</span>, <span class="math inline">\(P_{1}=P_{2}\)</span> on avg</td>
</tr>
<tr class="odd">
<td>Observational, good artificial control</td>
<td><span class="math inline">\(\overline{P}_{1}^{A}\)</span> vs <span class="math inline">\(\overline{P}_{2}^{B}\)</span>, <span class="math inline">\(P_{1}=P_{2}\)</span> hopefully on avg</td>
</tr>
<tr class="even">
<td>Observational, poor artificial control</td>
<td><span class="math inline">\(\overline{P}_{1}^{A}\)</span> vs <span class="math inline">\(\overline{P}_{2}^{B}\)</span>, <span class="math inline">\(P_{1}\neq P_{2}\)</span> on avg</td>
</tr>
<tr class="odd">
<td>Real-world physician practice</td>
<td><span class="math inline">\(P_{1}^{A}\)</span> vs <span class="math inline">\(P_{2}^{B}\)</span></td>
</tr>
</tbody>
</table>
<p>The best experimental designs yield the best evidence a clinician needs to answer the “what if” therapeutic question for the one patient in front of her.</p>
<p>Regarding adherence, proponents of “real world” evidence advocate for estimating treatment effects in the context of making treatment adherence low as in clinical practice. This would result in lower efficacy and the abandonment of many treatments. It is hard to argue that a treatment should not be available for a potentially adherent patient because her fellow patients were poor adherers. Note that an RCT is the best hope for estimating efficacy as a function of adherence, through for example an instrumental variable analysis (the randomization assignment is a truly valid instrument). Much more needs to be said about how to handle treatment adherence and what should be the target adherence in an RCT, but overall it is a good thing that RCTs do not mimic clinical practice. We are entering a new era of pragmatic clinical trials. Pragmatic trials are worthy of in-depth discussion, but it is not a stretch to say that the chief advantage of pragmatic trials is not that they provide results that are more relevant to clinical practice but that they are cheaper and faster than traditional randomized trials.</p>
